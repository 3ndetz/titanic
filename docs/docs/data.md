# Data

Всё про данные в проекте.

## Содержание

- [Data](#data)
  - [Содержание](#содержание)
  - [Текущие данные в проекте](#текущие-данные-в-проекте)
    - [Структура папок](#структура-папок)
    - [Текущие данные](#текущие-данные)
  - [Загрузка новых данных](#загрузка-новых-данных)
  - [Версионирование данных](#версионирование-данных)

## Текущие данные в проекте

### Структура папок

Данные хранятся в папке `data/` в корне проекта.

Структура папок данных следующая:

```text
data/
├── raw/               <- Сырые данные из источников
│   ├── *.csv           <- Сырые табличные данные
│   ├── *.dvc           <- Трекер dvc
│   └── *_metadata.yaml <- Схема данных
├── external/          <- Данные из сторонних источников
├── interim/           <- Промежуточные преобразованные данные
├── processed/         <- Обработанные данные для моделей
│   ├── *.parquet       <- Обработанные табличные данные
│   └── *_metadata.yaml <- Схема данных
├── features/          <- Сгенерированные фичи для моделей
└── models/            <- Чекпоинты моделей
```

Также для удобства работы с данными в IDE предусмотрены pydantic-модели данных.

```text
titanic/
└── schema/
    └── dataset_schema.py <- Pydantic-модели для табличных данных
```

Во всех этих подпапках, кроме `models` должны храниться данные.
В `raw` допускается `csv`, в остальных используем `parquet` для эффективности.

Каждый файл данных в папках `raw` и `processed` должен иметь рядом файл с метаданными в формате yaml, описывающий схему данных в [pandera](https://pandera.readthedocs.io/en/latest/schema_inference.html)-совместимом формате.

### Текущие данные

Изначальные сырые данные скачаны вручную из [Kaggle](https://www.kaggle.com/competitions/titanic/data) и помещены в `data/raw/`.

> Более красивым способом будет автоматизация через Kaggle и скачивание командой

Далее с данными была проведена работа, см. раздел ниже.

## Загрузка новых данных

Перед любыми манипуляциями с данными нужно активировать окружение и установить зависимости, про это есть в [getting started](./getting-started.md).

Чтобы добавить новые данные в проект, нужно:

1. Положить новые сырые данные `.csv` в папку `data/raw/`.
2. Подготовить схемы данных в формате, совместимом с pandera schema и создать соответствующие yaml-файлы:
   1. `data/raw/<new_dataset>_metadata.yaml` - для сырых данных
   2. `data/processed/<new_dataset>_metadata.yaml` - для обработанных данных
   3. Создать pydantic-модель для валидации строк новых данных в `titanic/schema/dataset_schema.py` для комфортной работе в коде.
3. При необходимости изменить обработку новых данных в `titanic/dataset.py`.

   Сейчас скрипт работает только с `train.csv`, для изменения необходимо изменить `dvc.yaml`:

   в разделе `process_data` добавить к `cmd: python titanic/dataset.py` аргументы-пути к новым данным, например:
   `cmd: python titanic/dataset.py data/raw/new_dataset.csv data/processed/new_dataset.parquet`.

4. Запустить команду `dvc repro process_data`, чтобы обработать новые данные и сохранить их в `data/processed/`.
5. Зафиксировать изменения в git, указав название коммита и эксперимента, и сделать dvc push.

## Версионирование данных

Данные версионируются с помощью DVC, подключенному к приложению Google Drive. Для доступа к этому диску сообщите разработчику gmail-почту, и тогда сможете пройти oauth при `dvc pull`.

Соответственно, `dvc pull` подтянет все данные из удалённого хранилища.
